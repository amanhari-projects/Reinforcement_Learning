{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "GAMMA = 0.8\n",
    "TEST_EPISODES = 20\n",
    "ENV = 'FrozenLake-v1'\n",
    "#ENV = 'FrozenLake8x8-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration Methord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent class\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)\n",
    "        self.cur_state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transition = collections.defaultdict(collections.Counter)\n",
    "        self.value = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_steps(self,count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()  # random action\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            self.rewards[(self.cur_state,action,new_state)] = reward\n",
    "            self.transition[(self.cur_state,action)][new_state]+=1\n",
    "            if done:\n",
    "                self.cur_state = self.env.reset()\n",
    "            else:\n",
    "                self.cur_state = new_state\n",
    "\n",
    "    def calc_state_action_value(self,state,action):\n",
    "        s_dash_states = self.transition[(state,action)]\n",
    "        total_count = sum(s_dash_states.values())\n",
    "        action_value = 0.0\n",
    "        for s_dash, count in s_dash_states.items():\n",
    "            reward = self.rewards[(state,action,s_dash)]\n",
    "            val = reward + GAMMA*self.value[s_dash]\n",
    "            action_value+= (count/total_count)*val\n",
    "        return action_value\n",
    "    \n",
    "    def best_action_select(self,state):\n",
    "        best_action = None\n",
    "        best_action_value = None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_state_action_value(state,action)\n",
    "            if best_action_value is None or best_action_value<action_value:\n",
    "                best_action_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "\n",
    "    def update_state_value(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [self.calc_state_action_value(state,action) \n",
    "            for action in range(self.env.action_space.n)]\n",
    "            self.value[state] = max(state_values)\n",
    "\n",
    "    def play_episode(self,env,render=False):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.best_action_select(state)\n",
    "            new_state,reward,done,_ = env.step(action)\n",
    "            self.rewards[(state,action,new_state)] = reward\n",
    "            self.transition[(state,action)][new_state]+=1\n",
    "            total_reward+=reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            state = new_state\n",
    "        env.close()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment='-v-iteration')\n",
    "    test_env = gym.make(ENV)\n",
    "\n",
    "    iter_no = 0.0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no+=1\n",
    "        print(\"Iteration number : \"+str(iter_no))\n",
    "        agent.play_n_steps(100)\n",
    "        agent.update_state_value()\n",
    "\n",
    "        reward = 0.0\n",
    "        for i in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\",reward,iter_no)\n",
    "        if reward>best_reward:\n",
    "            best_reward = reward\n",
    "            print(\"Reward updated : \"+str(best_reward))\n",
    "        if reward>0.9:\n",
    "            print(\"Env solved in \"+str(iter_no))\n",
    "            break\n",
    "    writer.close()\n",
    "    agent.play_episode(test_env,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent class\n",
    "\n",
    "class Q_Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)\n",
    "        self.cur_state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transition = collections.defaultdict(collections.Counter)\n",
    "        self.q_value = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_steps(self,count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()  # random action\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            self.rewards[(self.cur_state,action,new_state)] = reward\n",
    "            self.transition[(self.cur_state,action)][new_state]+=1\n",
    "            if done:\n",
    "                self.cur_state = self.env.reset()\n",
    "            else:\n",
    "                self.cur_state = new_state\n",
    "\n",
    "    def best_action_select(self,state):\n",
    "        best_action = None\n",
    "        best_action_value = None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.q_value[(state,action)]\n",
    "            if best_action_value is None or best_action_value<action_value:\n",
    "                best_action_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "\n",
    "    def update_q_table(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                action_value = 0.0\n",
    "                s_dash_states = self.transition[(state,action)]\n",
    "                total_count = sum(s_dash_states.values())\n",
    "                for s_dash, count in s_dash_states.items():\n",
    "                    reward = self.rewards[state,action,s_dash]\n",
    "                    best_action = self.best_action_select(state)\n",
    "                    val = reward + GAMMA*self.q_value[(s_dash,best_action)]\n",
    "                    action_value+=(count/total_count)*val\n",
    "                self.q_value[(state,action)] = action_value\n",
    "\n",
    "    def play_episode(self,env,render=False,video_frames = None):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.best_action_select(state)\n",
    "            new_state,reward,done,_ = env.step(action)\n",
    "            self.rewards[(state,action,new_state)] = reward\n",
    "            self.transition[(state,action)][new_state]+=1\n",
    "            total_reward+=reward\n",
    "            state = new_state\n",
    "            if render:\n",
    "                video_frames.append(env.render(mode='rgb_array'))\n",
    "        env.close()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = []\n",
    "if __name__==\"__main__\":\n",
    "    agent = Q_Agent()\n",
    "    writer = SummaryWriter(comment='-v-iteration')\n",
    "    test_env = gym.make(ENV)\n",
    "\n",
    "    \n",
    "    iter_no = 0.0\n",
    "    best_reward = float('-inf')\n",
    "    while True:\n",
    "        iter_no+=1\n",
    "        print(\"Iteration number : \"+str(iter_no))\n",
    "        agent.play_n_steps(100)\n",
    "        agent.update_q_table()\n",
    "\n",
    "        reward = 0.0\n",
    "        for i in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\",reward,iter_no)\n",
    "        print('Reward : '+str(reward))\n",
    "        if reward>best_reward:\n",
    "            best_reward = reward\n",
    "            print(\"Reward updated : \"+str(best_reward))\n",
    "        if reward>0.9:\n",
    "            print(\"Env solved in \"+str(iter_no))\n",
    "            break\n",
    "    writer.close()\n",
    "    agent.play_episode(test_env,True,video_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "print(len(video_frames))\n",
    "arr = np.asarray(video_frames)\n",
    "size = (256,256)\n",
    "out = cv2.VideoWriter('project_brown.mp4',cv2.VideoWriter_fourcc(*'DIVX'),15,size)\n",
    "for i in range(len(video_frames)):\n",
    "    rgb_img = cv2.cvtColor(arr[i],cv2.COLOR_RGB2BGR)\n",
    "    out.write(rgb_img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7205b945efee62e5b0116856ff9d12c5ab8dfb84871ec2b5eae9276e519a0707"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
