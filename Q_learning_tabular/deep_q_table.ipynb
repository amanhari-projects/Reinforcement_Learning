{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "GAMMA = 0.8\n",
    "TEST_EPISODES = 20\n",
    "ENV = 'FrozenLake-v1'\n",
    "#ENV = 'FrozenLake8x8-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration Methord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent class\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)\n",
    "        self.cur_state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transition = collections.defaultdict(collections.Counter)\n",
    "        self.value = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_steps(self,count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()  # random action\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            self.rewards[(self.cur_state,action,new_state)] = reward\n",
    "            self.transition[(self.cur_state,action)][new_state]+=1\n",
    "            if done:\n",
    "                self.cur_state = self.env.reset()\n",
    "            else:\n",
    "                self.cur_state = new_state\n",
    "\n",
    "    def calc_state_action_value(self,state,action):\n",
    "        s_dash_states = self.transition[(state,action)]\n",
    "        total_count = sum(s_dash_states.values())\n",
    "        action_value = 0.0\n",
    "        for s_dash, count in s_dash_states.items():\n",
    "            reward = self.rewards[(state,action,s_dash)]\n",
    "            val = reward + GAMMA*self.value[s_dash]\n",
    "            action_value+= (count/total_count)*val\n",
    "        return action_value\n",
    "    \n",
    "    def best_action_select(self,state):\n",
    "        best_action = None\n",
    "        best_action_value = None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_state_action_value(state,action)\n",
    "            if best_action_value is None or best_action_value<action_value:\n",
    "                best_action_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "\n",
    "    def update_state_value(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [self.calc_state_action_value(state,action) \n",
    "            for action in range(self.env.action_space.n)]\n",
    "            self.value[state] = max(state_values)\n",
    "\n",
    "    def play_episode(self,env,render=False):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.best_action_select(state)\n",
    "            new_state,reward,done,_ = env.step(action)\n",
    "            self.rewards[(state,action,new_state)] = reward\n",
    "            self.transition[(state,action)][new_state]+=1\n",
    "            total_reward+=reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            state = new_state\n",
    "        env.close()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment='-v-iteration')\n",
    "    test_env = gym.make(ENV)\n",
    "\n",
    "    iter_no = 0.0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no+=1\n",
    "        print(\"Iteration number : \"+str(iter_no))\n",
    "        agent.play_n_steps(100)\n",
    "        agent.update_state_value()\n",
    "\n",
    "        reward = 0.0\n",
    "        for i in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\",reward,iter_no)\n",
    "        if reward>best_reward:\n",
    "            best_reward = reward\n",
    "            print(\"Reward updated : \"+str(best_reward))\n",
    "        if reward>0.9:\n",
    "            print(\"Env solved in \"+str(iter_no))\n",
    "            break\n",
    "    writer.close()\n",
    "    agent.play_episode(test_env,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabular Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent class\n",
    "\n",
    "class Q_Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)\n",
    "        self.cur_state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transition = collections.defaultdict(collections.Counter)\n",
    "        self.q_value = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_steps(self,count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()  # random action\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            self.rewards[(self.cur_state,action,new_state)] = reward\n",
    "            self.transition[(self.cur_state,action)][new_state]+=1\n",
    "            if done:\n",
    "                self.cur_state = self.env.reset()\n",
    "            else:\n",
    "                self.cur_state = new_state\n",
    "\n",
    "    def best_action_select(self,state):\n",
    "        best_action = None\n",
    "        best_action_value = None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.q_value[(state,action)]\n",
    "            if best_action_value is None or best_action_value<action_value:\n",
    "                best_action_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "    \n",
    "\n",
    "    def update_q_table(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                action_value = 0.0\n",
    "                s_dash_states = self.transition[(state,action)]\n",
    "                total_count = sum(s_dash_states.values())\n",
    "                for s_dash, count in s_dash_states.items():\n",
    "                    reward = self.rewards[state,action,s_dash]\n",
    "                    best_action = self.best_action_select(state)\n",
    "                    val = reward + GAMMA*self.q_value[(s_dash,best_action)]\n",
    "                    action_value+=(count/total_count)*val\n",
    "                self.q_value[(state,action)] = action_value\n",
    "\n",
    "    def play_episode(self,env,render=False,video_frames = None):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.best_action_select(state)\n",
    "            new_state,reward,done,_ = env.step(action)\n",
    "            self.rewards[(state,action,new_state)] = reward\n",
    "            self.transition[(state,action)][new_state]+=1\n",
    "            total_reward+=reward\n",
    "            state = new_state\n",
    "            if render:\n",
    "                video_frames.append(env.render(mode='rgb_array'))\n",
    "        env.close()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number : 1.0\n",
      "Reward : 0.0\n",
      "Reward updated : 0.0\n",
      "Iteration number : 2.0\n",
      "Reward : 0.0\n",
      "Iteration number : 3.0\n",
      "Reward : 0.0\n",
      "Iteration number : 4.0\n",
      "Reward : 0.0\n",
      "Iteration number : 5.0\n",
      "Reward : 0.0\n",
      "Iteration number : 6.0\n",
      "Reward : 0.0\n",
      "Iteration number : 7.0\n",
      "Reward : 0.0\n",
      "Iteration number : 8.0\n",
      "Reward : 0.0\n",
      "Iteration number : 9.0\n",
      "Reward : 0.0\n",
      "Iteration number : 10.0\n",
      "Reward : 0.0\n",
      "Iteration number : 11.0\n",
      "Reward : 0.0\n",
      "Iteration number : 12.0\n",
      "Reward : 0.0\n",
      "Iteration number : 13.0\n",
      "Reward : 0.0\n",
      "Iteration number : 14.0\n",
      "Reward : 0.0\n",
      "Iteration number : 15.0\n",
      "Reward : 0.0\n",
      "Iteration number : 16.0\n",
      "Reward : 0.0\n",
      "Iteration number : 17.0\n",
      "Reward : 0.0\n",
      "Iteration number : 18.0\n",
      "Reward : 0.0\n",
      "Iteration number : 19.0\n",
      "Reward : 0.0\n",
      "Iteration number : 20.0\n",
      "Reward : 0.0\n",
      "Iteration number : 21.0\n",
      "Reward : 0.0\n",
      "Iteration number : 22.0\n",
      "Reward : 0.0\n",
      "Iteration number : 23.0\n",
      "Reward : 0.0\n",
      "Iteration number : 24.0\n",
      "Reward : 0.0\n",
      "Iteration number : 25.0\n",
      "Reward : 0.0\n",
      "Iteration number : 26.0\n",
      "Reward : 0.0\n",
      "Iteration number : 27.0\n",
      "Reward : 0.0\n",
      "Iteration number : 28.0\n",
      "Reward : 0.0\n",
      "Iteration number : 29.0\n",
      "Reward : 0.0\n",
      "Iteration number : 30.0\n",
      "Reward : 0.0\n",
      "Iteration number : 31.0\n",
      "Reward : 0.0\n",
      "Iteration number : 32.0\n",
      "Reward : 0.0\n",
      "Iteration number : 33.0\n",
      "Reward : 0.0\n",
      "Iteration number : 34.0\n",
      "Reward : 0.0\n",
      "Iteration number : 35.0\n",
      "Reward : 0.0\n",
      "Iteration number : 36.0\n",
      "Reward : 0.0\n",
      "Iteration number : 37.0\n",
      "Reward : 0.0\n",
      "Iteration number : 38.0\n",
      "Reward : 0.0\n",
      "Iteration number : 39.0\n",
      "Reward : 0.0\n",
      "Iteration number : 40.0\n",
      "Reward : 0.0\n",
      "Iteration number : 41.0\n",
      "Reward : 0.0\n",
      "Iteration number : 42.0\n",
      "Reward : 0.0\n",
      "Iteration number : 43.0\n",
      "Reward : 0.0\n",
      "Iteration number : 44.0\n",
      "Reward : 0.0\n",
      "Iteration number : 45.0\n",
      "Reward : 0.4\n",
      "Reward updated : 0.4\n",
      "Iteration number : 46.0\n",
      "Reward : 0.2\n",
      "Iteration number : 47.0\n",
      "Reward : 0.0\n",
      "Iteration number : 48.0\n",
      "Reward : 0.2\n",
      "Iteration number : 49.0\n",
      "Reward : 0.1\n",
      "Iteration number : 50.0\n",
      "Reward : 0.65\n",
      "Reward updated : 0.65\n",
      "Iteration number : 51.0\n",
      "Reward : 0.1\n",
      "Iteration number : 52.0\n",
      "Reward : 0.05\n",
      "Iteration number : 53.0\n",
      "Reward : 0.1\n",
      "Iteration number : 54.0\n",
      "Reward : 0.5\n",
      "Iteration number : 55.0\n",
      "Reward : 0.3\n",
      "Iteration number : 56.0\n",
      "Reward : 0.55\n",
      "Iteration number : 57.0\n",
      "Reward : 0.1\n",
      "Iteration number : 58.0\n",
      "Reward : 0.55\n",
      "Iteration number : 59.0\n",
      "Reward : 0.1\n",
      "Iteration number : 60.0\n",
      "Reward : 0.6\n",
      "Iteration number : 61.0\n",
      "Reward : 0.15\n",
      "Iteration number : 62.0\n",
      "Reward : 0.2\n",
      "Iteration number : 63.0\n",
      "Reward : 0.15\n",
      "Iteration number : 64.0\n",
      "Reward : 0.4\n",
      "Iteration number : 65.0\n",
      "Reward : 0.75\n",
      "Reward updated : 0.75\n",
      "Iteration number : 66.0\n",
      "Reward : 0.3\n",
      "Iteration number : 67.0\n",
      "Reward : 0.4\n",
      "Iteration number : 68.0\n",
      "Reward : 0.2\n",
      "Iteration number : 69.0\n",
      "Reward : 0.2\n",
      "Iteration number : 70.0\n",
      "Reward : 0.2\n",
      "Iteration number : 71.0\n",
      "Reward : 0.45\n",
      "Iteration number : 72.0\n",
      "Reward : 0.15\n",
      "Iteration number : 73.0\n",
      "Reward : 0.4\n",
      "Iteration number : 74.0\n",
      "Reward : 0.05\n",
      "Iteration number : 75.0\n",
      "Reward : 0.5\n",
      "Iteration number : 76.0\n",
      "Reward : 0.1\n",
      "Iteration number : 77.0\n",
      "Reward : 0.25\n",
      "Iteration number : 78.0\n",
      "Reward : 0.15\n",
      "Iteration number : 79.0\n",
      "Reward : 0.55\n",
      "Iteration number : 80.0\n",
      "Reward : 0.0\n",
      "Iteration number : 81.0\n",
      "Reward : 0.25\n",
      "Iteration number : 82.0\n",
      "Reward : 0.1\n",
      "Iteration number : 83.0\n",
      "Reward : 0.45\n",
      "Iteration number : 84.0\n",
      "Reward : 0.2\n",
      "Iteration number : 85.0\n",
      "Reward : 0.35\n",
      "Iteration number : 86.0\n",
      "Reward : 0.55\n",
      "Iteration number : 87.0\n",
      "Reward : 0.25\n",
      "Iteration number : 88.0\n",
      "Reward : 0.4\n",
      "Iteration number : 89.0\n",
      "Reward : 0.1\n",
      "Iteration number : 90.0\n",
      "Reward : 0.5\n",
      "Iteration number : 91.0\n",
      "Reward : 0.1\n",
      "Iteration number : 92.0\n",
      "Reward : 0.45\n",
      "Iteration number : 93.0\n",
      "Reward : 0.3\n",
      "Iteration number : 94.0\n",
      "Reward : 0.3\n",
      "Iteration number : 95.0\n",
      "Reward : 0.7\n",
      "Iteration number : 96.0\n",
      "Reward : 0.3\n",
      "Iteration number : 97.0\n",
      "Reward : 0.3\n",
      "Iteration number : 98.0\n",
      "Reward : 0.25\n",
      "Iteration number : 99.0\n",
      "Reward : 0.1\n",
      "Iteration number : 100.0\n",
      "Reward : 0.35\n",
      "Iteration number : 101.0\n",
      "Reward : 0.35\n",
      "Iteration number : 102.0\n",
      "Reward : 0.15\n",
      "Iteration number : 103.0\n",
      "Reward : 0.5\n",
      "Iteration number : 104.0\n",
      "Reward : 0.4\n",
      "Iteration number : 105.0\n",
      "Reward : 0.55\n",
      "Iteration number : 106.0\n",
      "Reward : 0.45\n",
      "Iteration number : 107.0\n",
      "Reward : 0.5\n",
      "Iteration number : 108.0\n",
      "Reward : 0.45\n",
      "Iteration number : 109.0\n",
      "Reward : 0.7\n",
      "Iteration number : 110.0\n",
      "Reward : 0.45\n",
      "Iteration number : 111.0\n",
      "Reward : 0.55\n",
      "Iteration number : 112.0\n",
      "Reward : 0.35\n",
      "Iteration number : 113.0\n",
      "Reward : 0.05\n",
      "Iteration number : 114.0\n",
      "Reward : 0.2\n",
      "Iteration number : 115.0\n",
      "Reward : 0.15\n",
      "Iteration number : 116.0\n",
      "Reward : 0.5\n",
      "Iteration number : 117.0\n",
      "Reward : 0.05\n",
      "Iteration number : 118.0\n",
      "Reward : 0.6\n",
      "Iteration number : 119.0\n",
      "Reward : 0.55\n",
      "Iteration number : 120.0\n",
      "Reward : 0.5\n",
      "Iteration number : 121.0\n",
      "Reward : 0.6\n",
      "Iteration number : 122.0\n",
      "Reward : 0.4\n",
      "Iteration number : 123.0\n",
      "Reward : 0.9\n",
      "Reward updated : 0.9\n",
      "Iteration number : 124.0\n",
      "Reward : 0.2\n",
      "Iteration number : 125.0\n",
      "Reward : 0.25\n",
      "Iteration number : 126.0\n",
      "Reward : 0.3\n",
      "Iteration number : 127.0\n",
      "Reward : 0.2\n",
      "Iteration number : 128.0\n",
      "Reward : 0.1\n",
      "Iteration number : 129.0\n",
      "Reward : 0.3\n",
      "Iteration number : 130.0\n",
      "Reward : 0.25\n",
      "Iteration number : 131.0\n",
      "Reward : 0.25\n",
      "Iteration number : 132.0\n",
      "Reward : 0.15\n",
      "Iteration number : 133.0\n",
      "Reward : 0.25\n",
      "Iteration number : 134.0\n",
      "Reward : 0.2\n",
      "Iteration number : 135.0\n",
      "Reward : 0.15\n",
      "Iteration number : 136.0\n",
      "Reward : 0.3\n",
      "Iteration number : 137.0\n",
      "Reward : 0.3\n",
      "Iteration number : 138.0\n",
      "Reward : 0.15\n",
      "Iteration number : 139.0\n",
      "Reward : 0.3\n",
      "Iteration number : 140.0\n",
      "Reward : 0.35\n",
      "Iteration number : 141.0\n",
      "Reward : 0.3\n",
      "Iteration number : 142.0\n",
      "Reward : 0.35\n",
      "Iteration number : 143.0\n",
      "Reward : 0.75\n",
      "Iteration number : 144.0\n",
      "Reward : 0.3\n",
      "Iteration number : 145.0\n",
      "Reward : 0.15\n",
      "Iteration number : 146.0\n",
      "Reward : 0.6\n",
      "Iteration number : 147.0\n",
      "Reward : 0.3\n",
      "Iteration number : 148.0\n",
      "Reward : 0.15\n",
      "Iteration number : 149.0\n",
      "Reward : 0.7\n",
      "Iteration number : 150.0\n",
      "Reward : 0.35\n",
      "Iteration number : 151.0\n",
      "Reward : 0.35\n",
      "Iteration number : 152.0\n",
      "Reward : 0.2\n",
      "Iteration number : 153.0\n",
      "Reward : 0.4\n",
      "Iteration number : 154.0\n",
      "Reward : 0.8\n",
      "Iteration number : 155.0\n",
      "Reward : 0.15\n",
      "Iteration number : 156.0\n",
      "Reward : 0.25\n",
      "Iteration number : 157.0\n",
      "Reward : 0.6\n",
      "Iteration number : 158.0\n",
      "Reward : 0.45\n",
      "Iteration number : 159.0\n",
      "Reward : 0.15\n",
      "Iteration number : 160.0\n",
      "Reward : 0.7\n",
      "Iteration number : 161.0\n",
      "Reward : 0.8\n",
      "Iteration number : 162.0\n",
      "Reward : 0.85\n",
      "Iteration number : 163.0\n",
      "Reward : 0.8\n",
      "Iteration number : 164.0\n",
      "Reward : 0.8\n",
      "Iteration number : 165.0\n",
      "Reward : 0.75\n",
      "Iteration number : 166.0\n",
      "Reward : 0.6\n",
      "Iteration number : 167.0\n",
      "Reward : 0.9\n",
      "Iteration number : 168.0\n",
      "Reward : 0.6\n",
      "Iteration number : 169.0\n",
      "Reward : 0.25\n",
      "Iteration number : 170.0\n",
      "Reward : 0.05\n",
      "Iteration number : 171.0\n",
      "Reward : 0.6\n",
      "Iteration number : 172.0\n",
      "Reward : 0.8\n",
      "Iteration number : 173.0\n",
      "Reward : 0.15\n",
      "Iteration number : 174.0\n",
      "Reward : 0.8\n",
      "Iteration number : 175.0\n",
      "Reward : 0.65\n",
      "Iteration number : 176.0\n",
      "Reward : 0.7\n",
      "Iteration number : 177.0\n",
      "Reward : 0.9\n",
      "Iteration number : 178.0\n",
      "Reward : 0.8\n",
      "Iteration number : 179.0\n",
      "Reward : 0.65\n",
      "Iteration number : 180.0\n",
      "Reward : 0.8\n",
      "Iteration number : 181.0\n",
      "Reward : 0.6\n",
      "Iteration number : 182.0\n",
      "Reward : 0.75\n",
      "Iteration number : 183.0\n",
      "Reward : 0.95\n",
      "Reward updated : 0.95\n",
      "Env solved in 183.0\n"
     ]
    }
   ],
   "source": [
    "video_frames = []\n",
    "if __name__==\"__main__\":\n",
    "    agent = Q_Agent()\n",
    "    writer = SummaryWriter(comment='-v-iteration')\n",
    "    test_env = gym.make(ENV)\n",
    "\n",
    "    \n",
    "    iter_no = 0.0\n",
    "    best_reward = float('-inf')\n",
    "    while True:\n",
    "        iter_no+=1\n",
    "        print(\"Iteration number : \"+str(iter_no))\n",
    "        agent.play_n_steps(100)\n",
    "        agent.update_q_table()\n",
    "\n",
    "        reward = 0.0\n",
    "        for i in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\",reward,iter_no)\n",
    "        print('Reward : '+str(reward))\n",
    "        if reward>best_reward:\n",
    "            best_reward = reward\n",
    "            print(\"Reward updated : \"+str(best_reward))\n",
    "        if reward>0.9:\n",
    "            print(\"Env solved in \"+str(iter_no))\n",
    "            break\n",
    "    writer.close()\n",
    "    agent.play_episode(test_env,True,video_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "print(len(video_frames))\n",
    "arr = np.asarray(video_frames)\n",
    "size = (256,256)\n",
    "out = cv2.VideoWriter('project_brown.mp4',cv2.VideoWriter_fourcc(*'DIVX'),15,size)\n",
    "for i in range(len(video_frames)):\n",
    "    rgb_img = cv2.cvtColor(arr[i],cv2.COLOR_RGB2BGR)\n",
    "    out.write(rgb_img)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7205b945efee62e5b0116856ff9d12c5ab8dfb84871ec2b5eae9276e519a0707"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
