{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# import global\n",
    "ENV   = 'FrozenLake-v1'\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "NUM_TEST_EP = 20\n",
    "video_frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward updated from 0.0 to 0.1\n",
      "Reward updated from 0.1 to 0.3\n",
      "Reward updated from 0.3 to 0.45\n",
      "Reward updated from 0.45 to 0.6\n",
      "Reward updated from 0.6 to 0.75\n",
      "Reward updated from 0.75 to 0.85\n",
      "Reward updated from 0.85 to 0.95\n",
      "Iteration complete at : 296\n"
     ]
    }
   ],
   "source": [
    "# Agent Class\n",
    "class Agent:\n",
    "    def __init__(self,env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.cur_state = self.env.reset()\n",
    "        self.q_table = collections.defaultdict(float)\n",
    "\n",
    "    # take a random single step in the enviornment\n",
    "    def random_step_env(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.cur_state\n",
    "        new_state, reward, done, _ = self.env.step(action=action)\n",
    "        if done:\n",
    "            self.cur_state = self.env.reset()\n",
    "        else:\n",
    "            self.cur_state = new_state\n",
    "        return old_state,action,reward,new_state\n",
    "    \n",
    "    # find best action in a given state\n",
    "    def best_action(self,state):\n",
    "        best_action,best_action_value = None,None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.q_table[(state,action)]\n",
    "            if best_action is None or best_action_value<action_value:\n",
    "                best_action = action\n",
    "                best_action_value = action_value\n",
    "        return best_action,best_action_value\n",
    "    \n",
    "    # update the q_table\n",
    "    def update_q_table(self,state,action,reward,new_state):\n",
    "        _, q_star = self.best_action(new_state)\n",
    "        new_value = reward + GAMMA*q_star\n",
    "        old_value = self.q_table[(state,action)]\n",
    "        self.q_table[(state,action)] = old_value * (1-ALPHA) + new_value * ALPHA\n",
    "    \n",
    "    # play an episode of the enviornment\n",
    "    def play_episode(self,env,render=False):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = self.best_action(state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = new_state\n",
    "            if render:\n",
    "                env.render()\n",
    "                video_frames.append(env.render(mode='rgb_array'))\n",
    "        return total_reward\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV)\n",
    "    agent = Agent(ENV)\n",
    "    writer = SummaryWriter(comment='-q-learning-tabular')\n",
    "\n",
    "    itr_num = 0\n",
    "    best_reward = 0.0\n",
    "    random_step_num = 100\n",
    "\n",
    "    while True:\n",
    "        itr_num+=1\n",
    "        for _ in range(random_step_num):\n",
    "            old_state,action,reward,new_state = agent.random_step_env()\n",
    "            agent.update_q_table(old_state,action,reward,new_state)\n",
    "        \n",
    "        reward = 0.0\n",
    "        for _ in range(NUM_TEST_EP):\n",
    "            reward+=agent.play_episode(test_env)\n",
    "        reward/=NUM_TEST_EP\n",
    "        writer.add_scalar(\"reward\",reward,itr_num)\n",
    "        if reward>best_reward:\n",
    "            print('Reward updated from '+str(best_reward)+' to '+str(reward))\n",
    "            best_reward = reward\n",
    "        if reward>0.9:\n",
    "            print('Iteration complete at : '+str(itr_num))\n",
    "            break\n",
    "    agent.play_episode(test_env,True)\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58564944/'DIVX' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "# Save the Episode as video\n",
    "import numpy as np\n",
    "import cv2\n",
    "print(len(video_frames))\n",
    "arr = np.asarray(video_frames)\n",
    "size = (256,256)\n",
    "out = cv2.VideoWriter('project_brown.mp4',cv2.VideoWriter_fourcc(*'DIVX'),15,size)\n",
    "for i in range(len(video_frames)):\n",
    "    rgb_img = cv2.cvtColor(arr[i],cv2.COLOR_RGB2BGR)\n",
    "    out.write(rgb_img)\n",
    "out.release()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7205b945efee62e5b0116856ff9d12c5ab8dfb84871ec2b5eae9276e519a0707"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
